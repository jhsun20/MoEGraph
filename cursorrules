rules:
  - description: Only use the GOOD benchmark for all training and evaluation.
  - description: Use only PyTorch and PyTorch Geometric (PyG) for all model and data handling.
  - description: Do not use DGL, TensorFlow, or any non-PyG graph libraries.
  - description: All model architectures (e.g., GCN, GIN, GraphSAGE) must be instantiated via a single factory function (e.g., get_model(config)) to support easy switching via config.yaml.
  - description: All experiments must be configurable through a centralized config.yaml file, including model type, number of experts, augmentation flags, gating strategy, and regularization options.
  - description: Do not hardcode dataset paths; use config-driven relative paths (e.g., config['dataset']['path']).
  - description: Every core module (e.g., augmentation, gating, aggregation) must be optional and controllable via parameters for easy ablation.
  - description: Naming convention for all Python files should use snake_case (e.g., expert_router.py, gating_module.py).
  - description: Class names must use PascalCase (e.g., GraphMoE, ExpertGNN).
  - description: All experiments should log to logs/, checkpoints to checkpoints/, and results to results/, with directory names encoding the key config values (e.g., gin_aug=true_experts=4/).
  - description: All evaluation scripts must output both in-distribution and OOD metrics.
  - description: Avoid using global variables; pass all necessary values through config or function arguments.
  - description: Every new feature added must include an associated config parameter, even if defaulted.
  - description: Prefer modular composition over inheritance for composing expert pipelines (e.g., using nn.ModuleList instead of class hierarchies).
  - description: Include a README.md in the root folder and each submodule with instructions and examples.
